
# USP SÃ£o Carlos â€“ Pipeline Automatizado de Indicadores Institucionais
[ğŸ‡ºğŸ‡¸ English](README_EN.md) | PortuguÃªs

![USP SÃ£o Carlos](https://img.shields.io/badge/USP-SÃ£o%20Carlos-0050A0)
![Projeto Institucional](https://img.shields.io/badge/Projeto-Institucional-blue)
![PUB-USP](https://img.shields.io/badge/PUB-USP%202024/2025-lightgrey)
![ETL](https://img.shields.io/badge/ETL-Produtivo-success)
![ValidaÃ§Ã£o](https://img.shields.io/badge/ValidaÃ§Ã£o-2019--2024-green)
![ReutilizÃ¡vel](https://img.shields.io/badge/ReutilizÃ¡vel-Anualmente-important)
![LicenÃ§a](https://img.shields.io/badge/LicenÃ§a-MIT-black)


Este repositÃ³rio contÃ©m um pipeline completo para automaÃ§Ã£o da coleta, filtragem e padronizaÃ§Ã£o dos principais indicadores do campus da USP SÃ£o Carlos, extraÃ­dos diretamente do AnuÃ¡rio EstatÃ­stico da USP (USP Digital).

O projeto foi desenvolvido no Ã¢mbito do Programa Unificado de Bolsas (PUB-USP) com o objetivo de substituir o processo manual de obtenÃ§Ã£o de dados institucionais por uma soluÃ§Ã£o reproduzÃ­vel, padronizada e facilmente reutilizÃ¡vel em ediÃ§Ãµes futuras do AnuÃ¡rio.

---

## Problema que o projeto resolve

O AnuÃ¡rio apresenta dados consolidados para toda a USP, o que obriga cada campus a realizar uma coleta manual para obter seus prÃ³prios indicadores.
Esse processo Ã© lento, suscetÃ­vel a erros, inconsistente entre anos e precisa ser refeito a cada nova ediÃ§Ã£o.

Este pipeline elimina esse problema ao:

- realizar a extraÃ§Ã£o direta das tabelas oficiais via HTTP;
- padronizar automaticamente colunas e formatos que variam entre ediÃ§Ãµes;
- filtrar exclusivamente os dados referentes ao campus SÃ£o Carlos;
- gerar arquivos finais prontos para anÃ¡lise em CSV;
- permitir a repetiÃ§Ã£o anual do processo com esforÃ§o praticamente nulo.

---

## AvanÃ§o em relaÃ§Ã£o ao cronograma oficial

De acordo com o cronograma previsto no edital do PUB-USP, o projeto estÃ¡ dividido em etapas distribuÃ­das ao longo de 12 meses, incluindo estudo das tabelas, desenvolvimento gradual dos mÃ³dulos, testes, documentaÃ§Ã£o e treinamento.

A execuÃ§Ã£o real iniciou-se em **04 de setembro**.  
Em menos de 3 meses trabalho, jÃ¡ foram concluÃ­dos:

- o mÃ³dulo completo de extraÃ§Ã£o;
- o mÃ³dulo completo de transformaÃ§Ã£o;
- o mÃ³dulo de carga;
- o pipeline integrado, testado e validado com as ediÃ§Ãµes de 2019 a 2024;
- a filtragem e padronizaÃ§Ã£o do campus SÃ£o Carlos;
- a estrutura final de ETL (Extract, Transform, Load) em formato reutilizÃ¡vel.

O pipeline Ã© totalmente genÃ©rico e independente do ano; as ediÃ§Ãµes 2019â€“2024 foram utilizadas apenas para validaÃ§Ã£o. O sistema estÃ¡ preparado para processar automaticamente quaisquer futuros AnuÃ¡rios, assim que forem publicados.

ConclusÃ£o: O motor de ETL estÃ¡ operacional 4 meses antes do previsto no cronograma oficial.

---

## Tech Stack
* **Linguagem:** Python
* **ManipulaÃ§Ã£o de Dados:** Pandas (DataFrames, Pivot Tables, Cleaning)
* **IngestÃ£o de Dados:** Requests (Web Scraping de arquivos estÃ¡ticos)
* **Compatibilidade Excel:** OpenPyXL, xlrd

---
## Arquitetura do projeto

```
usp_sao_carlos_data_pipeline/
â”‚
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ extract.py        # Download das tabelas diretamente do USP Digital
â”‚   â”œâ”€â”€ transform.py      # PadronizaÃ§Ã£o de colunas e filtragem de SÃ£o Carlos
â”‚   â”œâ”€â”€ load.py           # Salvamento dos resultados em CSV
â”‚   â””â”€â”€ pipeline.py       # OrquestraÃ§Ã£o completa do processo
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # (opcional) tabelas brutas, caso queira armazenar
â”‚   â””â”€â”€ processed/        # CSVs finais prontos para anÃ¡lise
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ usp_sao_carlos_data_pipeline.ipynb   # Notebook original usado no protÃ³tipo
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ PUB-Projeto.pdf   # Documento oficial com o cronograma e descriÃ§Ã£o do projeto
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ export_notebook_to_py.py             # ConversÃ£o do notebook para .py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```


## Sobre a pasta `data/raw`

A pasta `data/raw` representa o local reservado para armazenar arquivos brutos baixados do AnuÃ¡rio, antes de qualquer filtragem ou transformaÃ§Ã£o.  
Ela faz parte da arquitetura padrÃ£o de projetos de engenharia de dados, especialmente quando hÃ¡ interesse em histÃ³rico, auditoria ou reprocessamento.

No entanto, como os arquivos brutos podem ser grandes e variam a cada ediÃ§Ã£o do AnuÃ¡rio, a pasta permanece **vazia no GitHub**.  
Isso evita que o repositÃ³rio se torne pesado e garante que apenas o cÃ³digo essencial permaneÃ§a versionado.

Localmente, o uso dessa pasta Ã© opcional. Ela pode ser utilizada para:
- armazenar as tabelas originais (`.xls` ou `.csv`) baixadas do USP Digital;  
- manter histÃ³rico de dados para validaÃ§Ã£o ou auditoria;  
- evitar downloads repetidos durante o desenvolvimento;  
- permitir reprocessamento offline.
